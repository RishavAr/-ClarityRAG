# ClarityRAG
 ClarityRAG:An Internal AI Assistant for Instant Confluence Knowledge Retrieval
 
Overview

ClarityRAG is a Retrieval-Augmented Generation (RAG) system built to make navigating internal documentation effortless and fast. It enables natural language querying of Confluence content using a local LLM pipeline, eliminating the need for keyword hunting or tribal knowledge.

📌 Problem Statement

Confluence pages in large organizations are abundant and inconsistently structured. Employees waste time manually searching or asking others for context that already exists. ClarityRAG solves this by providing a chat-based AI interface that retrieves and summarizes relevant internal content instantly.
🎯 Features

🔍 Semantic search over internal docs (Confluence exports or custom markdown)
💬 Chat interface with streaming support
🧠 Local inference engine powered by your own model (clarity-rag)
🛠️ FastAPI backend with OpenAI-compatible /v1/chat/completions and /v1/chat/completions-stream routes
🧾 Modular RAG pipeline with retrieval → context injection → generation
🌐 Web UI support (Open WebUI or Postman)
🏠 Self-hosted, private, and secure
🏗️ Architecture

        +------------------+        +------------------------+
        |  User (Web UI)   | <----> |  FastAPI Inference API |
        +------------------+        +-----------+------------+
                                              |
                                              v
                             +----------------+----------------+
                             |   RAG Pipeline:                  |
                             |   - Retriever (Vector DB)        |
                             |   - Prompt Assembler             |
                             |   - Generator (LLM: clarity-rag) |
                             +----------------------------------+
                                              |
                                              v
                                 +-------------------------+
                                 | Streamed Response Output |
                                 +-------------------------+
⚙️ How to Set Up Locally (New Machine Setup)

1. Clone the Repository
git clone https://github.com/yourname/clarity-rag.git
cd clarity-rag
2. Create & Activate Python Environment
python3 -m venv rag-env
source rag-env/bin/activate
3. Install Dependencies
pip install -r requirements.txt
4. Run the FastAPI App
uvicorn abh:app --host 0.0.0.0 --port 8000
5. Connect WebUI or Postman
Web UI (e.g., Open WebUI):
Endpoint: http://<your-ip>:8000/v1/chat/completions (or /completions-stream for streaming)
Model ID: clarity-rag
Enable “Stream Chat Response” for live feedback
🧪 API Endpoints

Method	Endpoint	Description
POST	/v1/chat/completions	Basic chat completion
POST	/v1/chat/completions-stream	Streaming response (token by token)
GET	/v1/api/version	[Optional] API health/version
GET	/v1/api/tags, /v1/api/ps	[Optional] UI metadata routes
🧠 Prompt & RAG Logic

Vector store built from markdown or .txt content
Semantic retrieval returns top-k matches
Context injected into prompt template
Response generated by local LLM with streaming support
🔧 Customization

Replace get_answer() in fi.py with your own RAG logic using LangChain or Haystack
Model swapping: Use any transformers or llama.cpp model you want, as long as it supports generation

📂 File Structure
ClarityRAG/
├── app.py              # FastAPI app
├── rag_engine.py       # Your RAG logic
├── utils.py            # Optional helper functions
├── faiss_index/        # Your FAISS index directory
├── models/             # Model weights (if local)
├── requirements.txt
├── Dockerfile
└── README.md


🚀 To Build and Run:
# Build the Docker image
docker build -t clarityrag .

# Run the container
docker run -p 8000:8000 clarityrag








