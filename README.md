# ClarityRAG
 ClarityRAG:An Internal AI Assistant for Instant Confluence Knowledge Retrieval
 
Overview

ClarityRAG is a Retrieval-Augmented Generation (RAG) system built to make navigating internal documentation effortless and fast. It enables natural language querying of Confluence content using a local LLM pipeline, eliminating the need for keyword hunting or tribal knowledge.

ğŸ“Œ Problem Statement

Confluence pages in large organizations are abundant and inconsistently structured. Employees waste time manually searching or asking others for context that already exists. ClarityRAG solves this by providing a chat-based AI interface that retrieves and summarizes relevant internal content instantly.
ğŸ¯ Features

ğŸ” Semantic search over internal docs (Confluence exports or custom markdown)
ğŸ’¬ Chat interface with streaming support
ğŸ§  Local inference engine powered by your own model (clarity-rag)
ğŸ› ï¸ FastAPI backend with OpenAI-compatible /v1/chat/completions and /v1/chat/completions-stream routes
ğŸ§¾ Modular RAG pipeline with retrieval â†’ context injection â†’ generation
ğŸŒ Web UI support (Open WebUI or Postman)
ğŸ  Self-hosted, private, and secure
ğŸ—ï¸ Architecture

        +------------------+        +------------------------+
        |  User (Web UI)   | <----> |  FastAPI Inference API |
        +------------------+        +-----------+------------+
                                              |
                                              v
                             +----------------+----------------+
                             |   RAG Pipeline:                  |
                             |   - Retriever (Vector DB)        |
                             |   - Prompt Assembler             |
                             |   - Generator (LLM: clarity-rag) |
                             +----------------------------------+
                                              |
                                              v
                                 +-------------------------+
                                 | Streamed Response Output |
                                 +-------------------------+

ğŸ“‚ Repository Structure


| Folder/File        | Description                                   |
| ------------------ | --------------------------------------------- |
| `rag_api.py`          | FastAPI server with chat endpoints            |
| `fi.py`      | Embedding logic using `sentence-transformers` |
| `requirements.txt` | Python dependencies                           |
| `Dockerfile`       | Container configuration                       |
| `docs/`            | Sample docs to be indexed                     |
| `README.md`        | You're reading it!                            |

âš™ï¸ How to Set Up Locally (New Machine Setup)

1. Clone the Repository

```bash 
git clone https://github.com/yourname/clarity-rag.git
cd clarity-rag
```

2. Create & Activate Python Environment
```bash

python3 -m venv rag-env
source rag-env/bin/activate
```

3. Install Dependencies
```bash

pip install -r requirements.txt
```

4. Run the FastAPI App
```bash

uvicorn abh:app --host 0.0.0.0 --port 8000
```

5. Connect WebUI or Postman
Web UI (Open WebUI):
Endpoint: http://<your-ip>:8000/v1/chat/completions (or /completions-stream for streaming)
Model ID: clarity-rag
Enable â€œStream Chat Responseâ€ for live feedback


ğŸ§ª API Endpoints

Method	Endpoint	Description
POST	/v1/chat/completions	Basic chat completion
POST	/v1/chat/completions-stream	Streaming response (token by token)
GET	/v1/api/version	[Optional] API health/version
GET	/v1/api/tags, /v1/api/ps	[Optional] UI metadata routes


ğŸ§  Prompt & RAG Logic

Vector store built from markdown or .txt content
Semantic retrieval returns top-k matches
Context injected into prompt template
Response generated by local LLM with streaming support
ğŸ”§ Customization

Replace get_answer() in fi.py with your own RAG logic using LangChain or Haystack
Model swapping: Use any transformers or llama.cpp model you want, as long as it supports generation



ğŸš€ To Build and Run:

# Build the Docker image
```bash

docker build -t clarityrag .
```
# Run the container
```bash
docker run -p 8000:8000 clarityrag
```









