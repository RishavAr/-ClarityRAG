# ClarityRAG
 ClarityRAG:An Internal AI Assistant for Instant Confluence Knowledge Retrieval
 
Overview

ClarityRAG is a Retrieval-Augmented Generation (RAG) system built to make navigating internal documentation effortless and fast. It enables natural language querying of Confluence content using a local LLM pipeline, eliminating the need for keyword hunting or tribal knowledge.

ğŸ“Œ Problem Statement

Confluence pages in large organizations are abundant and inconsistently structured. Employees waste time manually searching or asking others for context that already exists. ClarityRAG solves this by providing a chat-based AI interface that retrieves and summarizes relevant internal content instantly.
ğŸ¯ Features

ğŸ” Semantic search over internal docs (Confluence exports or custom markdown)
ğŸ’¬ Chat interface with streaming support
ğŸ§  Local inference engine powered by your own model (clarity-rag)
ğŸ› ï¸ FastAPI backend with OpenAI-compatible /v1/chat/completions and /v1/chat/completions-stream routes
ğŸ§¾ Modular RAG pipeline with retrieval â†’ context injection â†’ generation
ğŸŒ Web UI support (Open WebUI or Postman)
ğŸ  Self-hosted, private, and secure
ğŸ—ï¸ Architecture

        +------------------+        +------------------------+
        |  User (Web UI)   | <----> |  FastAPI Inference API |
        +------------------+        +-----------+------------+
                                              |
                                              v
                             +----------------+----------------+
                             |   RAG Pipeline:                  |
                             |   - Retriever (Vector DB)        |
                             |   - Prompt Assembler             |
                             |   - Generator (LLM: clarity-rag) |
                             +----------------------------------+
                                              |
                                              v
                                 +-------------------------+
                                 | Streamed Response Output |
                                 +-------------------------+
âš™ï¸ How to Set Up Locally (New Machine Setup)

1. Clone the Repository
git clone https://github.com/yourname/clarity-rag.git
cd clarity-rag
2. Create & Activate Python Environment
python3 -m venv rag-env
source rag-env/bin/activate
3. Install Dependencies
pip install -r requirements.txt
4. Run the FastAPI App
uvicorn abh:app --host 0.0.0.0 --port 8000
5. Connect WebUI or Postman
Web UI (e.g., Open WebUI):
Endpoint: http://<your-ip>:8000/v1/chat/completions (or /completions-stream for streaming)
Model ID: clarity-rag
Enable â€œStream Chat Responseâ€ for live feedback
ğŸ§ª API Endpoints

Method	Endpoint	Description
POST	/v1/chat/completions	Basic chat completion
POST	/v1/chat/completions-stream	Streaming response (token by token)
GET	/v1/api/version	[Optional] API health/version
GET	/v1/api/tags, /v1/api/ps	[Optional] UI metadata routes
ğŸ§  Prompt & RAG Logic

Vector store built from markdown or .txt content
Semantic retrieval returns top-k matches
Context injected into prompt template
Response generated by local LLM with streaming support
ğŸ”§ Customization

Replace get_answer() in fi.py with your own RAG logic using LangChain or Haystack
Model swapping: Use any transformers or llama.cpp model you want, as long as it supports generation

ğŸ“‚ File Structure
ClarityRAG/
â”œâ”€â”€ app.py              # FastAPI app
â”œâ”€â”€ rag_engine.py       # Your RAG logic
â”œâ”€â”€ utils.py            # Optional helper functions
â”œâ”€â”€ faiss_index/        # Your FAISS index directory
â”œâ”€â”€ models/             # Model weights (if local)
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ Dockerfile
â””â”€â”€ README.md


ğŸš€ To Build and Run:
# Build the Docker image
docker build -t clarityrag .

# Run the container
docker run -p 8000:8000 clarityrag








