# ClarityRAG
 ClarityRAG:An Internal AI Assistant for Instant Confluence Knowledge Retrieval
 
Overview

ClarityRAG is a Retrieval-Augmented Generation (RAG) system built to make navigating internal documentation effortless and fast. It enables natural language querying of Confluence content using a local LLM pipeline, eliminating the need for keyword hunting or tribal knowledge.

📌 Problem Statement

Confluence pages in large organizations are abundant and inconsistently structured. Employees waste time manually searching or asking others for context that already exists. ClarityRAG solves this by providing a chat-based AI interface that retrieves and summarizes relevant internal content instantly.
🎯 Features

🔍 Semantic search over internal docs (Confluence exports or custom markdown)
💬 Chat interface with streaming support
🧠 Local inference engine powered by your own model (clarity-rag)
🛠️ FastAPI backend with OpenAI-compatible /v1/chat/completions and /v1/chat/completions-stream routes
🧾 Modular RAG pipeline with retrieval → context injection → generation
🌐 Web UI support (Open WebUI or Postman)
🏠 Self-hosted, private, and secure
🏗️ Architecture

        +------------------+        +------------------------+
        |  User (Web UI)   | <----> |  FastAPI Inference API |
        +------------------+        +-----------+------------+
                                              |
                                              v
                             +----------------+----------------+
                             |   RAG Pipeline:                  |
                             |   - Retriever (Vector DB)        |
                             |   - Prompt Assembler             |
                             |   - Generator (LLM: clarity-rag) |
                             +----------------------------------+
                                              |
                                              v
                                 +-------------------------+
                                 | Streamed Response Output |
                                 +-------------------------+

📂 Repository Structure


| Folder/File        | Description                                   |
| ------------------ | --------------------------------------------- |
| `rag_api.py`          | FastAPI server with chat endpoints            |
| `fi.py`      | Embedding logic using `sentence-transformers` |
| `requirements.txt` | Python dependencies                           |
| `Dockerfile`       | Container configuration                       |
| `docs/`            | Sample docs to be indexed                     |
| `README.md`        | You're reading it!                            |

⚙️ How to Set Up Locally (New Machine Setup)

1. Clone the Repository

```bash 
git clone https://github.com/yourname/clarity-rag.git
cd clarity-rag
```

2. Create & Activate Python Environment
```bash

python3 -m venv rag-env
source rag-env/bin/activate
```

3. Install Dependencies
```bash

pip install -r requirements.txt
```

4. Run the FastAPI App
```bash

uvicorn abh:app --host 0.0.0.0 --port 8000
```

5. Connect WebUI or Postman
Web UI (Open WebUI):
Endpoint: http://<your-ip>:8000/v1/chat/completions (or /completions-stream for streaming)
Model ID: clarity-rag
Enable “Stream Chat Response” for live feedback


🧪 API Endpoints

Method	Endpoint	Description
POST	/v1/chat/completions	Basic chat completion
POST	/v1/chat/completions-stream	Streaming response (token by token)
GET	/v1/api/version	[Optional] API health/version
GET	/v1/api/tags, /v1/api/ps	[Optional] UI metadata routes


🧠 Prompt & RAG Logic

Vector store built from markdown or .txt content
Semantic retrieval returns top-k matches
Context injected into prompt template
Response generated by local LLM with streaming support
🔧 Customization

Replace get_answer() in fi.py with your own RAG logic using LangChain or Haystack
Model swapping: Use any transformers or llama.cpp model you want, as long as it supports generation



🚀 To Build and Run:

# Build the Docker image
```bash

docker build -t clarityrag .
```
# Run the container
```bash
docker run -p 8000:8000 clarityrag
```









